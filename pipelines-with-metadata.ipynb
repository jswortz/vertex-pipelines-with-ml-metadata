{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-cloud-pipeline-components --user\n",
    "# ! pip install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import component\n",
    "from typing import  NamedTuple,  List, Dict\n",
    "import json\n",
    "from kfp.dsl import Dataset, Input, Model, Output\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.dsl import component, Artifact \n",
    "from typing import NamedTuple\n",
    "import datetime\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "import time\n",
    "\n",
    "epoch_time = int(time.time())\n",
    "\n",
    "#ML Metadata types https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/artifact_types.html\n",
    "from google_cloud_pipeline_components.types.artifact_types import BQTable \n",
    "# We will also build a custom artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'cpg-cdp'\n",
    "BUCKET_NAME = f'jsw-artifact-{PROJECT_ID}'\n",
    "BUCKET = f'gs://{BUCKET_NAME}'\n",
    "YEAR = 2016\n",
    "ARTIFACT_BLOB = f'artifacts/baseball-{YEAR}'\n",
    "DATASET_ID = 'google_trends_my_project' #blank created BQ dataset\n",
    "OUTPUT_TABLE_NAME = f'baseball-schedule-{YEAR}'\n",
    "PIPELINE_ROOT = f\"{BUCKET}/pipeline_root/brandwatch_ingest_{now}\"\n",
    "PIPELINE_ROOT = PIPELINE_ROOT.replace(' ', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a bucket to store the pandas artifact\n",
    "This is to demonstrate a custom artifact\n",
    "\n",
    "Use [this guide](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/artifact_types.html) for the documentation on specialized platform metadata\n",
    "\n",
    "Note that standard components will produce the proper metadata, use this technique when building custom KFP components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil mb -l us-central1 $BUCKET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Component to create a BQ Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=[\n",
    "            'google.cloud.bigquery'\n",
    "            ,'google.cloud.storage'\n",
    "            ,'google_cloud_pipeline_components'\n",
    "            ],\n",
    ")\n",
    "def pull_baseball_data(\n",
    "    year: int,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    output_table_name: str,\n",
    ") -> NamedTuple('outputs'\n",
    "                , year=int\n",
    "                , bq_table=Artifact):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google_cloud_pipeline_components.types.artifact_types import BQTable \n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    output_table_uri = f'{project_id}.{dataset_id}.{output_table_name}'\n",
    "    query = f\"\"\"CREATE OR REPLACE TABLE `{output_table_uri}` AS (\n",
    "                SELECT * FROM `bigquery-public-data.baseball.schedules` WHERE year = {year}\n",
    "            )\n",
    "            \"\"\"\n",
    "    get_baseball_by_year_job = client.query(query)\n",
    "    get_baseball_by_year_job.result()\n",
    "    # make sure name = none so Vertex sets the name to the ML datastore\n",
    "    bq_table = BQTable.create(name = None\n",
    "                              , project_id = project_id\n",
    "                              , dataset_id = dataset_id\n",
    "                              , table_id = output_table_name\n",
    "                            )\n",
    "\n",
    "    return (year\n",
    "        , bq_table\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Component to consume the BQ Artifact and write a generic artifact to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=[\n",
    "            'google.cloud.bigquery'\n",
    "            ,'google.cloud.storage'\n",
    "            ,'pandas'\n",
    "            ,'google_cloud_pipeline_components'\n",
    "            ,'db-dtypes'\n",
    "            ],\n",
    ")\n",
    "def save_schedule_to_gcs(\n",
    "    bq_table: Input[BQTable],\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    destination_blob_name: str,\n",
    ") -> NamedTuple('outputs'\n",
    ", csv_data=Artifact):\n",
    "    from google.cloud import bigquery, storage\n",
    "    from kfp.dsl import Artifact \n",
    "\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    input_table_uri = f\"{bq_table.metadata['projectId']}.{bq_table.metadata['datasetId']}.{bq_table.metadata['tableId']}\" #using the artifact properties\n",
    "    query = f\"SELECT gameId, homeTeamName, awayTeamName, startTime FROM `{input_table_uri}`\"\n",
    "    \n",
    "    get_baseball_by_year_job = client.query(query)\n",
    "    result_df = get_baseball_by_year_job.result().to_dataframe()\n",
    "\n",
    "    #save the data frame to gcs and produce an artifact\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_string(result_df.to_csv(), 'text/csv')\n",
    "\n",
    "    csv_data = Artifact(name=None\n",
    "                            , uri = f'gs://{bucket_name}/{destination_blob_name}')\n",
    "    \n",
    "        \n",
    "    return (csv_data ,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v0_2'\n",
    "NAME = 'ml_metadata_simple_' + VERSION\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    year: int = YEAR,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    dataset_id: str = DATASET_ID,\n",
    "    output_table_name: str = OUTPUT_TABLE_NAME,\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    destination_blob_name: str = ARTIFACT_BLOB\n",
    "):\n",
    "\n",
    "    pull_baseball_data_op = pull_baseball_data(\n",
    "        year = year,\n",
    "        project_id = project_id,\n",
    "        dataset_id = dataset_id,\n",
    "        output_table_name = output_table_name\n",
    "    ).set_display_name(\"Pull Public Baseball Data Schedules\")\n",
    "\n",
    "    save_schedule_pandas_gcs_op = save_schedule_to_gcs(\n",
    "        bq_table = pull_baseball_data_op.outputs['bq_table'],\n",
    "        project_id = project_id,\n",
    "        bucket_name = bucket_name,\n",
    "        destination_blob_name = destination_blob_name\n",
    "    ).set_display_name(\"Save the csv data to GCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"artifact_pipeline_example.json\".replace(\" \", \"_\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=\"Getting artifacts with baseball data\",\n",
    "    template_path=\"artifact_pipeline_example.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    "    parameter_values={\n",
    "            'year': YEAR,\n",
    "            'project_id': PROJECT_ID,\n",
    "            'dataset_id': DATASET_ID,\n",
    "            'output_table_name': OUTPUT_TABLE_NAME,\n",
    "            'bucket_name': BUCKET_NAME,\n",
    "            'destination_blob_name': ARTIFACT_BLOB\n",
    "            },\n",
    "    failure_policy=\"slow\" #in case one of the many parallel/async components fails, it all stops\n",
    ")\n",
    "SERVICE_ACCOUNT = \"social-pulse-pipeline@cpg-cdp.iam.gserviceaccount.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/939655404703/locations/us-central1/pipelineJobs/ml-metadata-simple-v0-2-20231201002822\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/939655404703/locations/us-central1/pipelineJobs/ml-metadata-simple-v0-2-20231201002822')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/ml-metadata-simple-v0-2-20231201002822?project=939655404703\n"
     ]
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
